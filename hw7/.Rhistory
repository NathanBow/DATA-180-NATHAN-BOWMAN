tokenize_ngrams
# Chunk 17
charVector <- news$headline_text
newscorpus <- corpus(charVector)
newscorpus <- corpus_reshape(newscorpus, to = "paragraphs")
newscorpus
# Chunk 18
news_dtm <- dfm(newscorpus,
remove_punct = TRUE, remove_symbol = TRUE, remove_numbers = TRUE, remove = c(stopwords("english")))
news_dtm <- dfm_remove(news_dtm, c('sa', '=', 'nt', 'qld', 'nsw', 'abc', 'news'))
news_dtm <- dfm_trim(news_dtm, min_termfreq = 50)
head(news_dtm, 6)
# Chunk 19
library("quanteda.textplots")
textplot_wordcloud(news_dtm)
# I was not surprised to see common words like "government," "world," and "new" in the headlines, but the term "unexpected" and "unprecedented" was surprising for me to see as a common word being used. These observation in the dataset may provide insights into recurring themes in the new dataset.
# Chunk 20
library("topicmodels")
library('tidytext')
# Chunk 21
library(quanteda)
k <- 8
news_topics <- convert(news_dtm, to = "topicmodels")
topic_model <- LDA(news_topics, method = "VEM", control = list(seed = 1234), k = 8)
terms(topic_model, 8)
# Chunk 22
#install.packages("reshape2")
library(reshape2)
library(dplyr)
library(ggplot2)
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
news_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
news_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
# Topic 1 seems to be related to politics with words like "government" and "election." Topic 2 appears to be about health and well-being with words like "health" and "pandemic." Topic 3 might be related to technology and innovation with words like "technology" and "innovation."
# Chunk 23
tidy_news <- tidy(topic_model, matrix = "gamma")
top_news <- tidy_news %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_news %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
mydf <- data.frame(as.matrix(news_topics))
mydf$id <- rownames(mydf)
topic1 <- mydf %>% filter(id =='text953' | id == 'text4556' | id == 'text1553' | id == 'text5065' | id == 'text48')
topic2 <- mydf %>% filter(id =='text4859' | id == 'text5024' | id == 'text8695' | id == 'text9858' | id == 'text11')
topic1 <- subset(topic1, select = -id)
topic2 <- subset(topic2, select = -id)
topic1 <- data.frame(t(topic1))
topic2 <- data.frame(t(topic2))
topic1[rowSums(topic1) > 0,]
topic2[rowSums(topic2) > 0,]
# The top documents for topic 1 seem to be related to politics as common words with highest gamma include "council", "year", and "australian." In topic 2, the most common words with highest gamma include "man," "charged," and "power," which show that topic 2 is also related to politics. The documents are assigned to topics 1 and 2, as shown in the data set and figures based on each text data. In conclusion, it does make sense these documents are assigned to topics 1 and 2, accordingly.
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
library(tidyverse)
library(tm)
news<-read.csv("/Users/anishachoudhury/Desktop/DATA 180 Anisha/DATA-180-Anisha/hw7/news.csv",header=T)
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
library(tidyverse)
library(tm)
news<-read.csv("D:/Dickinson College Teaching/DATA 180/hw7/news.csv",header=T)
head(news)
# Chunk 3
posWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
sort(unique(news$year))
# Chunk 5
charVector <- news$headline_text
head(charVector)
# Chunk 6
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
# Chunk 7
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower)) #lowercase
wordCorpus <- tm_map(wordCorpus, removePunctuation) #remove punctutation
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
stop_words<-stopwords("english")
#stop_words
wordCorpus[["1"]][["content"]]
# Chunk 8
tdm <- TermDocumentMatrix(wordCorpus)
tdm
# Chunk 9
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
head(wordCounts,10)
# Chunk 10
barplot(wordCounts[wordCounts >= 50], xlab = "Words", ylab = "Frequency", cex.names = 0.75,las = 2)
# Chunk 11
totalWords <-sum(wordCounts)
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP>20],las=2,cex.names=0.75)
sum(matchedP)/totalWords
#percent: 3.7%
matchedP <- match(names(wordCounts), negWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP>20],las=2,cex.names=0.75)
sum(matchedP)/totalWords
# percent:7.7%
# Chunk 12
library(dplyr)
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
# Chunk 13
library(ggplot2)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() + labs(title = "Frequency of Articles Released by Year and Month", x = "Year/Month", y = "Number of Articles Released") + theme(axis.text=element_text(size=4,angle=90))
# Chunk 14
library("quanteda")
# Chunk 15
x<-termFreq(charVector,control=list(removePunctuation=TRUE,stopwords=TRUE))
sort(x, decreasing = TRUE)[1:20]
# Chunk 16
library(tokenizers)
#tokenize_words(charVector)
#tokenize_ngrams(charVector, n=2)
#install.packages("ngram")
library(ngram)
words <-paste(unlist(charVector), collapse=" ")
ng <- ngram(words, n=2)
head(get.phrasetable(ng),20)
# Chunk 17
newscorpus <- corpus(charVector)
paras <- corpus_reshape(newscorpus, to="paragraphs")
paras
# Chunk 18
news_dtm <- dfm(paras, stem=TRUE, remove_punct=TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove=c(stopwords("english")))
#news_dtm
news_dtm <- dfm_remove(news_dtm,c('=','nt','nsw','abc','qld','news','sa'))
news_dtm <- dfm_trim(news_dtm, min_termfreq=50)
head(news_dtm)
# Chunk 19
library("quanteda.textplots")
textplot_wordcloud(news_dtm)
# Chunk 20
library("topicmodels")
library('tidytext')
# Chunk 21
Topics <- convert(news_dtm, to = "topicmodels")
topic_model <- LDA(Topics, method = "VEM", control=list(seed=1234), k=8)
terms(topic_model,8)
# Chunk 22
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
news_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
news_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
# Chunk 23
mydf <- data.frame(as.matrix(Topics))
mydf$id <- rownames(mydf)
topic1 <- mydf %>% filter(id=='text5727' | id=='text3694' | id=='text5685' | id=='text3566' | id=='text4585')
topic2 <- mydf %>% filter(id=='text2762' | id=='text7307' | id=='text489' | id=='text8416' | id=='text3610')
topic1 <- subset(topic1, select = -id )
topic2 <- subset(topic2, select = -id )
topic1 <- data.frame(t(topic1))
topic2 <- data.frame(t(topic2))
topic1[rowSums(topic1)>0,]
topic2[rowSums(topic2)>0,]
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#library(tidyverse)
library(tm)
news<-read.csv("D:/Dickinson College Teaching/DATA 180/hw7/news.csv",header=T)
# Chunk 3
posWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
sort(unique(news$year))
# Chunk 5
charVector <- news$headline_text
head(charVector)
# Chunk 6
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
# Chunk 7
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus[["1"]][["content"]]
# Chunk 8
tdm <- TermDocumentMatrix(wordCorpus)
# Chunk 9
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts,10)
# Chunk 10
barplot(wordCounts[wordCounts>50],las=2,cex.names = 0.75)
# Chunk 11
totalWords <- sum(wordCounts)
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
barplot(matchedP[matchedP>20],las=2,cex.names=0.75)
sum(matchedP)/totalWords
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
matchedN <- matchedN != 0
matchedN <- wordCounts[matchedN]
barplot(matchedN[matchedN>20],las=2,cex.names=0.75)
sum(matchedN)/totalWords
# Chunk 12
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
news
# Chunk 13
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() +theme(axis.text=element_text(size=4,angle=90))
# Chunk 14
library("quanteda")
x<-termFreq(charVector)
sort(x, decreasing = TRUE)[1:20]
# Chunk 15
mostFreq <- termFreq(charVector)
sort(mostFreq,decreasing = TRUE)[1:20]
# Chunk 16
library(tokenizers)
tokenize_words(charVector)
tokenize_ngrams(charVector, n =2)
library(ngram)
characters <- paste(unlist(charVector), collapse = " ")
ng <- ngram(characters, n = 2)
head(get.phrasetable(ng),20)
# Chunk 17
newscorpus <- corpus(charVector)
paras <- corpus_reshape(newscorpus, to="paragraphs")
# Chunk 18
news_dtm <- dfm(paras, stem=TRUE, remove_punct=TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove=c(stopwords("english")))
news_dtm <- dfm_remove(news_dtm,c('sa','=','nt','qld','nsw','abc','news'))
news_dtm <- dfm_trim(news_dtm, min_termfreq=50)
head(news_dtm)
# Chunk 19
library("quanteda.textplots")
textplot_wordcloud(news_dtm)
# Chunk 20
library("topicmodels")
library('tidytext')
# Chunk 21
news_topics <- convert(news_dtm, to = "topicmodels")
topic_model <- LDA(news_topics, method = "VEM", control=list(seed=1234), k=8)
terms(topic_model,8)
# Chunk 22
# Beta is the probability that a given term appears in a particular topic.
# Higher probability terms "define" the topic best.
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
news_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>% # cool func, gets the max n for each topic group
ungroup() %>% # to get the tibble without group tag
arrange(topic, -beta)
news_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>% # this hack is to order for facet
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") + # scales="free" allows x-y scales to be free.
scale_y_reordered() # used in combo with reorder_within
# Chunk 23
tidy_news <- tidy(topic_model, matrix = "gamma")
# gamma plots
top_news <- tidy_news %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_news %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
library(tidyverse)
library(tm)
news<-read.csv("D:/Dickinson College Teaching/DATA 180/hw7/news.csv",header=T)
news
# Chunk 3
posWords <- scan(
"D:/Dickinson College Teaching/DATA 180/hw7/positive-words.txt"
, character(0), sep = "\n")  # 2006 items
negWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
range <- range(news$year)
year_range = range[2] - range[1]
year_range
# Chunk 5
charVector <- news$headline_text
head(charVector)
charVector
# Chunk 6
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
#wordCorpus
# Chunk 7
tm_map(wordCorpus, tolower)
tm_map(wordCorpus, removeNumbers)
tm_map(wordCorpus, removePunctuation)
tm_map(wordCorpus, removeWords, stopwords("english"))
# Chunk 8
# Term document matrix is a mathematical matrix that represents the frequency of terms in a collection of document. It also represents the relationship between terms and documents, where each row stands for a term, and each column for a document, nd an entry is the number of occurrences of the terms in the documents.
tdm <- TermDocumentMatrix(wordCorpus)
# Chunk 9
m <- as.matrix(tdm)
dim(m)
wordCounts <- rowSums(m)
#wordCounts
wordFrequency <- sort(wordCounts, decreasing = TRUE)
topWords <- head(wordFrequency, 10)
topWords
# Chunk 10
#selectedWords <- findFreqTerms(tdm, lowfreq = 50)
#selectedWords
barplot(wordCounts[wordFrequency >= 50], las = 2, cex.names = 0.75, angle = 90)
# Chunk 11
# POSITIVE WORDS
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- wordCounts[matchedP != 0]
percentagePos <- (sum(matchedP)/sum(wordCounts))*100
cat("The percentage of positive words in the news headlines:", percentagePos)
matchedPfreq <- matchedP[matchedP >= 20]
barplot(matchedPfreq, las = 2, cex.names = 0.75)
# NEGATIVE WORDS
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
matchedN <- wordCounts[matchedN != 0]
percentageNeg <- (sum(matchedN)/sum(wordCounts))*100
cat("The percentage of negative words in the news headlines:", percentageNeg)
matchedNfreq <- matchedN[matchedN >= 20]
barplot(matchedNfreq, las = 2, cex.names = 0.75)
# Chunk 12
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
#news
# Chunk 13
library(ggplot2)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() + labs(title = "Frequency of Articles Released by Month", x = "Month Year", y = "Number of Articles Released") +  theme(axis.text=element_text(size=4,angle=90))
# Chunk 14
library("quanteda")
#library('corpus')
# Chunk 15
head(sort(termFreq(charVector), decreasing = TRUE), 20)
# Chunk 16
#library(tokenizers)
#tokenize_words(charVector)
#head(tokenize_ngrams(charVector, n = 2), 20)
install.packages("ngram")
library(ngram)
words <- paste(unlist(charVector), collapse =" ")
ng <- ngram(words, n = 2)
head(get.phrasetable(ng),20)
# Chunk 17
newscorpus <- corpus(charVector)
paragraphs <- corpus_reshape(newscorpus)
paragraphs
# Chunk 18
news_dtm <- dfm(paragraphs, stem = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove = c(stopwords("english")))
news_dtm <- dfm_remove(news_dtm, c("s", "?", "thi"))
news_dtm <- dfm_trim(news_dtm, min_termfreq = 50)
head(colnames(news_dtm))
# Chunk 19
library("quanteda.textplots")
wordCloud <- textplot_wordcloud(news_dtm)
# I'm not surprised seeing the words such as: polic, plan, report, new, say, call, interview, or court since these are the common words in the news. However, I'm surprised that words like: win, us, charg, water, or wa appear pretty much on the news headlines.
# Chunk 20
#install.packages("reshape2")
#install.packages("ggplot2")
#library('reshape2')
library("topicmodels")
library('tidytext')
library("dplyr")
library("ggplot2")
# Chunk 21
news_topics <- convert(news_dtm, to = "topicmodels")
topic_model <- LDA(news_topics, method = "VEM", k = 8)
terms(topic_model, 10)
# Chunk 22
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
news_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
news_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
# In the graph, words are classified into different groups with different topics.
# Topic 1 and topic 2 seem to be associated with crime or violent incidents. The words "kill," "say," and "charg" suggest a focus on reporting incidents, potentially criminal in nature, while the word "new" is a bit generic and might be context-dependent, and "nation" suggests that this is national issue.
# Topic 3 could involve claims or assertions, potentially related to local governance ("council"), market-related issues, and health matters.
# Topic 4 might be associated with legal proceedings ("court"), planning activities, law enforcement ("polic"), and Australian-related news.
# Chunk 23
topic_model <- LDA(news_topics, method = "VEM", k = 8)
terms(topic_model, 5)
tidy_topics <- tidy(topic_model, matrix = "gamma")
tidy_news <- tidy_topics %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_news %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered()
# The topics seem to align with the expected themes based on the top words and the content of the top documents.
install.packages("ngram")
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#library(tidyverse)
library(tm)
news<-read.csv("D:/Dickinson College Teaching/DATA 180/hw7/news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
news$year <- as.integer(format(as.Date(news$publish_date, format = "%Y%m%d"), "%Y"))
charVector <- news$headline_text
cat("First 6 entries in charVector:\n")
cat(charVector[1:6], "\n")
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
cat("Preview of wordCorpus:\n")
print(wordCorpus)
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("en"))
cat("Text after Trimming:\n")
cat(as.character(wordCorpus[[1]]), "\n")
tdm <- TermDocumentMatrix(wordCorpus)
cat("Term Document Matrix Summary:\n")
print(tdm)
tdm <- TermDocumentMatrix(wordCorpus)
cat("Term Document Matrix Summary:\n")
print(tdm)
